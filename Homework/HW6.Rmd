---
title: "Netflix Challenge"
output: html_document
---


**This homework is due Monday April 18, 2016 at 11:59PM EST. When complete, submit your code in an R Markdown file and the knitted HTML via GitHub.**

Recommendation systems use rating data from many products 
and users to make recommendations for a specific user. 
Netflix uses a recommendation system to predict your ratings
for a specific movie.

On October 2006 Netflix offered a challenge to the data science
community: improve our recommendation algorithm by 10% and
win a million dollars. In September 2009 
[the winners were announced](http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/). You can read a good summary of how the winning algorithm was put together [here](http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/) 
and a more detailed explanation [here](http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf).

![winners](http://graphics8.nytimes.com/images/2009/09/21/technology/netflixawards.480.jpg)

In this homework, you will build your own recommendation system.
You will submit predicted recommendations for a test data set 
where we have kept the actual recommendations hidden. The test data set 
that you will have to predict is [available on GitHub here](https://github.com/datasciencelabs/data/blob/master/movielens-test.csv.gz). 
We will then check your performance and have our own Netflix 
challenge. You are allowed to form group. The winning team, 
defined by the best root mean squared error (RMSE), 
will receive a prize. Along the way we will learn about 
regularization and PCA to estimate hidden factors.

In the problems below, we will ask you to try out different models. 
Keep track of the RMSE you get for each one because we will ask 
you to report it at the end.





## Problem 1


### Problem 1A
Load the [compressed train data set available on GitHub](https://github.com/datasciencelabs/data/blob/master/movielens-train.csv.gz) into R. Call the data set `ratings`. 
How many users are there in the `ratings` data set? 
How many movies are in the `ratings` data set? 
What is the lowest rating in the data set? The highest? 

**Hint**: You can read in compressed file with `read_csv(gzfile(filename))`

```{r,warning=FALSE, message=FALSE}
library(readr)
download.file('https://media.githubusercontent.com/media/datasciencelabs/data/master/movielens-train.csv.gz', destfile = "movielens-train.csv.gz" ) 
filename <- "movielens-train.csv.gz" 
ratings <-read_csv(gzfile(filename)) #508 MB
gc() # make sure memory is clear
```


### Problem 1B

What is the median number of movies each user has rated? 
What is the maximum number of movies rated by a single user? 

```{r,warning=FALSE, message=FALSE }
library(dplyr)
 
# First, I want to see if user have multiple ratings for the same movie
ratings %>% 
        group_by(userId)%>%
        summarise(dist_movie_n_user=n_distinct(movieId), movie_n_user=n()) %>%
        mutate(diff=movie_n_user-dist_movie_n_user) %>%
        filter(diff>0)

# No, every user rated a movie at most once.

# Count the number of movies per user, and summarize median and maximum number of movies per user
ratings %>% 
        group_by(userId)%>%
        summarise(movie_n_user=n_distinct(movieId)) %>%
        ungroup() %>%
        summarise(min_n=min(movie_n_user), median_n=median(movie_n_user), max_n=max(movie_n_user))

# Count total number of movies in the dataset
ratings %>% summarise(total_movie_n=n_distinct(movieId))

```

Comment on these results and how they compare to the total
number of movies in the data set.

**Your answer here**: 

People's rating behavior vary a lot. Some people tend to watch and rate a lot of movies, far beyond average movie goers, with the maximum rated 26% of all the movies (8821 out of 33670), they could be die-hard fans of movies and could be the target of recommendation; a typical or average user rated only 0.08% of all the movie availabel (27 out of 33670).


Make a plot to visualize the distribution of number of movies per user.

```{r,warning=FALSE, message=FALSE }
#Make a plot to visualize the distribution of number of movies per user.
library(ggplot2)
ratings %>% 
        group_by(userId)%>%
        summarise(movie_n_user=n_distinct(movieId)) %>%
        ungroup() %>%
        ggplot(aes(movie_n_user,col='pink',fill=T)) +
        geom_density() + 
        theme(legend.position='none')+
        ggtitle("Distribution of Number of Movies per User") +
        xlab("Log10 Number of Movies per User")+
        geom_vline(xintercept =8821)+
        annotate("text", x = 8821, y = 1, label = "Maximum 8821 Movies per User",col='purple',size=5)+
        scale_x_log10()  
        
```


### Problem 1C

Now that we know some basic facts about our data set, 
let's randomly split the data into training and test data. 
Use `set.seed(755)` and the `sample()` function to select 
your test index to create two separate data frames
called: `train` and `test` from the original `ratings` data frame. 
Please have `test` contain a randomly selected 10% of the rows 
of `ratings`, and have `train` contain the other 90%. We will 
use these data frames to do the rest of the analyses in the problem set.
After you create `train` and `test`, you can remove `ratings` 
to save space. 


```{r,warning=FALSE, message=FALSE }
set.seed(755)
n_test <- round(nrow(ratings)*0.1)
test_index <- sample(1:nrow(ratings), n_test, replace=FALSE)

test <- ratings[test_index,]
train <- ratings[-test_index,]
 
rm(ratings)
gc()

```


### Problem 1D
Write a function called `RMSE()` that takes two numeric vectors
(one corresponding to the true movie ratings, and one 
corresponding to predicted movie ratings) as input, and returns 
the root mean square error (RMSE) between the two as output. 
The definition of root mean square error is square root of the 
average of the residuals squared:

$$\mbox{RMSE} = \sqrt{\frac{1}{N}\sum_{i=1}^N (\hat{Y}_i - Y_i)^2}$$

where $Y_i$ is the true $i$-th movie rating and 
$\hat{Y}_i$ our predicted rating. 
We can interpret this similarly to a standard deviation.
It is the typical error we make when predicting a movie rating.

Verify that `RMSE(true_ratings=c(4,3,3,4), predicted_ratings=c(4.5,3.5,1,4))` is 1.06.

```{r,warning=FALSE, message=FALSE }
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings-predicted_ratings)^2))
}

RMSE(true_ratings=c(4,3,3,4), predicted_ratings=c(4.5,3.5,1,4))

```

RMSE was the metric used to judge entries in the Netflix challenge.
The lower the RMSE was on Netflix's quiz set between the submitted 
rating predictions and the actual ratings, the better the method was. 
We will be using RMSE to evaluate our machine learning models in 
this homework as well.


-----


## Problem 2


### Problem 2A
Our goal here is to fit a model using the `train` data that we 
can use to predict user ratings for the movies in the `test` data. 
To begin, let's fit the simplest possible model: suppose we 
decided to do no statistical work at all and simply predict 
every single rating to be the average rating in the training set. 

What is the RMSE on the test set using this naive model?

```{r,warning=FALSE, message=FALSE }
# calculate the average rating in the training set
mu <-mean(train$rating)
cat("The average rating in the training set= \n",mu)

# If we predict all unknown ratins to be the average rating in the training set
naive_RMSE <- RMSE(true_ratings=test$rating, predicted_ratings=rep(mu, nrow(test)))

# place-holder for all models RMSE
RMSE_results <- data.frame(Method="Just the Average", RMSE=naive_RMSE)

# View Model RMSE table
library(knitr)
RMSE_results %>% kable
```


### Problem 2B

We get a RMSE of about 1. To win the grand prize of $1,000,000, 
a participating team had to get an RMSE of about 0.857. 
So we can definitely do better! 

We have information about individual users and about individual 
movies. Some movies are likely just better than other movies 
(more highly-rated on average), and some users are probably more 
critical of movies than other users (give lower ratings on average). 
Let's take advantage of that information.

Let's start with a simple model that accounts for differences 
between movies. In Problem 2A, we predicted all ratings with the 
same value, the mean. To account for the different ratings we would 
model the observed rating $Y_{u,i}$ for user $u$ and movie $i$ 
like this:

$$
Y_{u,i} = \mu + \varepsilon_{u,i}
$$

where $\mu$ is the average rating and $\varepsilon_{u,i}$ a random 
term explaining the differences between ratings.

But we know that some movies are better than others. So we can 
add a term $b_i$ to account for this and augment the model to 

$$
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
$$

In statistics we usually call the $b$s as effects, but in 
the Netflix challenge papers they refer to them as "bias" 
thus the $b$ notation.

To make a prediction we need to estimate $b_i$. 
We could use the `lm()` function to obtain least square estimates. 
However, from Problem 1A we know that there are over 20,000 movies
in our `train` data set.
The `lm()` function is not really meant for such scenarios. 
There are tools in R for such situation, but because we know that the 
least square estimate is just the average of $Y_{u,i} - \hat{\mu}$ 
for each movie, where $\hat{\mu}$ the average computed in
Problem 2A, we can simply use these averages.

Compute an estimate $\hat{b}_i$ for each $b_i$. Visualize and 
describe the distribution of the $\hat{b}_i$. Then, create a 
prediction with $\hat{\mu} + \hat{b}_i$ and calculate the RMSE.

**Hint 1**: Use `group_by()` and `summarize()` to estimate the $b_i$ 
and then one of the `join` functions to create a predictor 
for each user/movie combination in the test set. 

**Hint 2**: When you go predict on the test set, some of the users 
in the test set are not in the train set. You will not be able to 
estimate $b_i$ for them. Use the `replace_na()` function to make 
these biases 0.

```{r,warning=FALSE, message=FALSE }
# calculate biases for each movie in the training set
movie_bias <- train %>%
                    group_by(movieId)%>%
                    summarise(b_i=mean(rating-mu))

# visualize movie biases 
movie_bias%>%
          ggplot(aes(b_i))+
          geom_histogram(aes(fill=T))+
          theme(legend.position='none') +
          ggtitle("Distribution of Movie Biases around average rating, in training set")+
          xlab("Movie Bias")

```

**Your answer here**: 

> Indeed, some movies are more highly-rated than others, while some are more worse-rated than others. On the two extremities, the lowest-rated movie has almost 0-star among all users; the highest-reated movie has almost 5-star among all users. This wide range of differences can't be ignored. 

> In statistics, we can think of this phenomena as heterogeneity, that is, movie ratings are not independent observations. After subtracting the average rating, the residuals for some movies are more likely to be positive, which means the movie is generally more favored than avergae movies, and vice versa. If we only use average rating to predict, we leave a lot of movie information out, and thus the prediction is not ideal. We should incorporate some modeling strategy that account for the movie heterogeneity.


```{r,warning=FALSE, message=FALSE }
library(tidyr)
# prediction
test_prediction <-test %>%
                       left_join(movie_bias, by='movieId') %>%
                       replace_na(list(b_i=0))%>%
                       mutate(predicted_ratings=mu+b_i)

# RMSE
model1_RMSE <-RMSE(true_ratings =test_prediction$rating , predicted_ratings=test_prediction$predicted_ratings )
 
RMSE_results <-bind_rows(RMSE_results, data_frame(Method="Movie Effect Model", RMSE=model1_RMSE))

# View Model RMSE table
library(knitr)
RMSE_results %>% kable
```

**Your answer here**: 

> The RMSE improves from 1.0603780 to 0.9547747.



### Problem 2C

In Problem 2B, we noted that the distribution for 
the $b_i$ had values as small as -3. Use 
[this file](https://raw.githubusercontent.com/datasciencelabs/data/master/movies.csv) 
to get the movie titles and add them to your data frame 
containing the $b_i$ movie biases.
What are the 10 best and 10 worst 
rated movies based on the bias estimate $b_i$? 
Report the number of users that rated each one. 

```{r,warning=FALSE, message=FALSE }
filename <-'https://raw.githubusercontent.com/datasciencelabs/data/master/movies.csv'
movie_title <-read_csv(filename)

# join movie title with movie biases
movie_bias <- movie_bias%>%
            left_join(movie_title, by='movieId')

# calculate number of users rated each movie in training set
n_user <- train %>% 
                group_by(movieId)%>%
                summarise(n_user=n())

# join number of users per movie with movie biases
movie_bias <- movie_bias %>% left_join(n_user, by='movieId')

# What are the 10 best and 10 worst rated movies based on the bias estimate b_i? Report the number of users that rated each one. 
movie_bias %>% 
           arrange(b_i)%>%
           slice(1:10)%>% 
           select(b_i, title, n_user) %>%
           kable(caption = "Top 10 worst rated movies based on the bias estimate")
 
movie_bias %>% 
           arrange(desc(b_i))%>%
           slice(1:10)%>% 
           select(b_i, title, n_user) %>%
           kable(caption = "Top 10 best rated movies based on the bias estimate")           
```

**Your answer here**: 

> My observation is that, those best or worst rated movies are all rated by 1 or 2 users, which are highly likely just noise. That is, we are very uncertain about those bias estimate.



### Problem 2D

In Problem 2C, we saw that the supposed "best" and "worst"
movies were rated by very few users. These movies were 
mostly obscure ones. This is because with just a few users,
we have more uncertainty. Therefore, larger estimates 
of $b_i$, negative or positive, are more likely. 
These are "noisy" estimates that we should not trust, 
especially when it comes to prediction. Large errors can 
increase our RMSE, so we would rather be conservative
when not sure.

In previous homeworks, we computed standard error and 
constructed confidence intervals to account for different 
levels of uncertainty. However, when making predictions we 
need one number not an interval. For this we introduce the 
concept of regularization.

Regularization permits us to penalize large estimates that 
come from small sample sizes. It has commonalities with the 
Bayesian approach that "shrunk" predictions. The general 
idea is to minimize a penalized least squares equation

$$\sum_{i=1}^I \sum_{u=1}^{n_i} (Y_{u,i} - \mu - b_i)^2 + \lambda \sum_{i=1}^I b_i^2$$

This leads to a regularized estimate that can be approximated with:
$$
\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} (Y_{u,i} - \hat{\mu})
$$

where $n_i$ is the number of ratings made for  movie $i$. The 
larger the $\lambda$ or the smaller the $n_i$, 
the more we "shrink" $\hat{b}_i(\lambda)$ to 0. 

Compute these regularized estimates of $b_i$ using 
$\lambda=5$. Then, look at the top 10 best and worst movies. 
Do the movies make more sense now? Then see if the RMSE gets better. 

```{r,warning=FALSE, message=FALSE }

lambda <- 5

mu <-mean(train$rating) # average movie rating

# bias estimate per movie, based on regularization
movie_bias_reg <- train %>%
                        group_by(movieId)%>%
                        summarise(b_i=sum(rating-mu)/(n()+lambda), n_user=n()) %>%
                        left_join(movie_title, by='movieId')

# What are the 10 best and 10 worst rated movies based on the bias estimate b_i? Report the number of users that rated each one. 
movie_bias_reg %>% 
           arrange(b_i)%>%
           slice(1:10)%>% 
           select(b_i, title, n_user) %>%
           kable(caption = "Top 10 worst rated movies based on the bias estimate")
 
movie_bias_reg %>% 
           arrange(desc(b_i))%>%
           slice(1:10)%>% 
           select(b_i, title, n_user) %>%
           kable(caption = "Top 10 best rated movies based on the bias estimate")         

```

**Your answer here**: 

> Now, the best rated and worst rated movies make sense, and they have quite substantial number of users rated, which gives us more confidence in our point estimates.

```{r,warning=FALSE, message=FALSE }
# prediction
test_prediction <-test %>%
                       left_join(movie_bias_reg, by='movieId') %>%
                       replace_na(list(b_i=0))%>%
                       mutate(predicted_ratings=mu+b_i)

# RMSE
model2_RMSE <-RMSE(true_ratings =test_prediction$rating , predicted_ratings=test_prediction$predicted_ratings )
 
RMSE_results <-bind_rows(RMSE_results, data_frame(Method="Movie Effect Model with Regularization Lambda=5", RMSE=model2_RMSE))

# View Model RMSE table
library(knitr)
RMSE_results %>% kable
```

**Your answer here**: 

> RMSE gets a little better from just Movie effect without regularization, but not much. Although big improvement from naive average estimate.






### Problem 2E

We have improved the RMSE substantially from our 
initial naive guess. What else can we do to improve? 
Let's compute the average rating for user $u$. 

```{r,warning=FALSE, message=FALSE }
# calculate the average rating for user u_i
user_ave <- train %>%
                  group_by(userId)%>%
                  summarise(b_u=mean(rating))

 

# visualize user avergaes
user_ave  %>%
          ggplot(aes(b_u))+
          geom_histogram(aes(fill=T))+
          theme(legend.position='none') +
          ggtitle("Distribution of User Average Ratins, in training set")+
          xlab("User Average")

```

Note that there is substantial variability across users 
as well. This means some users are harsher than others 
and implies that a further improvement to our model may be:

$$ 
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
$$

where is $b_u$ a user-specific effect. Now it is possible 
that some users appear to be harsher than others only 
because they rate underaverage movies. For this reason we 
prefer to estimate $b_u$ using the residuals.

$$
\hat{b}_u = \frac{1}{n_u} \sum_{i=1}^{n_u} (Y_{u,i} - \hat{\mu} - \hat{b}_i)
$$

where $\hat{\mu}$ and $\hat{b}_i$ are calculated in the previous 
problems. Note that in theory we can use the least squares 
estimate provided by `lm()` but this approach will crash our 
computer. Furthermore, we prefer the regularized estimate of 
$b_u$. As with the movies, you will find that the harshest 
and kindest raters will be those that rated few movies and 
thus produce more uncertain estimates. We will therefore 
also regularize the user bias estimate:

$$
\hat{b}_u = \frac{1}{\alpha + n_u} \sum_{i=1}^{n_u} (Y_{u,i} - \hat{\mu} - \hat{b}_i)
$$

Let $\alpha=10$. Estimate the user effects $\hat{b}_u$. 
Then predict ratings and compute an RMSE. Note that although 
we are not requiring it here, we can use cross-validation 
to pick the $\alpha$s.

```{r,warning=FALSE, message=FALSE }
# Recall from previous question: Movie effect with regularization
lambda <- 5
mu <-mean(train$rating) # average movie rating
# bias estimate per movie, based on regularization
movie_bias_reg <- train %>%
                        group_by(movieId)%>%
                        summarise(b_i=sum(rating-mu)/(n()+lambda), n_user=n()) %>%
                        left_join(movie_title, by='movieId')

# Now, User Effect with regularization
alpha <-10
user_bias_reg <- train %>%
                       left_join(movie_bias_reg, by='movieId')%>%
                       group_by(userId)%>%
                       summarise(b_u=sum(rating-mu-b_i)/(n()+alpha), n_movie=n())

# prediction
test_prediction <-test %>%
                       left_join(movie_bias_reg, by='movieId') %>%
                       left_join(user_bias_reg, by="userId") %>%
                       replace_na(list(b_i=0, b_u=0))%>%
                       mutate(predicted_ratings=mu+b_i+b_u)

# RMSE
model3_RMSE <-RMSE(true_ratings =test_prediction$rating , predicted_ratings=test_prediction$predicted_ratings )
 
RMSE_results <-bind_rows(RMSE_results, data_frame(Method="Movie and User Effect Model with Regularization", RMSE=model3_RMSE))

# View Model RMSE table
library(knitr)
RMSE_results %>% kable

```

**Your answer here**: 

> Our prediction improved a lot, from adding user effect with regularization. This validated our observation that, there is substantial variability across users as well, some users are harsher than others.


-----



## Problem 3

Another common strategy for ratings prediction is matrix 
decomposition. The winning team used this approach and is described [here](http://www.netflixprize.com/assets/ProgressPrize2008_BellKor.pdf).
This approach is very much related to the PCA described in 
class. This problem will demonstrate how to use PCA to uncover
broad, latent patterns in user/movie relationships, and how to 
use the results of PCA to predict unknown user/movie relationships.

The general idea comes from the realization that there are 
clusters of movies that some people will like and others people 
won't. For example, there may be age divides, gender divides, 
"sophistication" divides, etc. This implies that a more complete 
model will include a term that is specific to groups of movies. 
For example, we let $X_{i,1} = 1$ if movie $i$ is a high budget
blockbuster and 0 otherwise. We can model each users effect:

$$ 
Y_{u,i} = \mu + b_i + b_u + b_{1,i} X_{i,1} + \varepsilon_{u,i}
$$

We may have other groupings: say $X_{i,2}=1$ if movie $i$ is a low 
brow comedy and 0 otherwise. Our model then gets augmented as 

$$ 
Y_{u,i} = \mu + b_i + b_u + b_{1,i} X_{i,1} + b_{2,i} X_{i,2} + \varepsilon_{u,i}
$$

But how do we find these $X$? If we think there are several
of these, then the model looks like:

$$ 
Y_{u,i} = \mu + b_i + b_u + \sum_{j=1}^p b_{j,i} X_{i,j}  + \varepsilon_{u,i}
$$

You can learn in a more advanced course that PCA can actually 
be used to estimate the $X$ that explain most variability. 
To do this we need to construct a matrix with movies in the rows 
and users in the columns. We can use the `spread()` function 
from `tidyr` to do this. However, if we just do this blindly 
we will get a matrix so large, that our computer will crash. 
Also, the matrix will have many NAs because not all users rate 
all movies. There are techniques to deal with these types of data. 
Here we will simplify the problem by looking at subset of the 
data with highly active users and movies that are rated by many.

> I think the idea behind matrix factoirzation is to generalize shared preferences to learn preferneces from statistical evidence. Reserach on preferences has tended to go beyond modeling individual choice, focusing on predicting which options people will like based not just on their own previous choice patterns (user effect) but also drawing on the choice of other people (movie effect). This work has led to the development of recommendation systems that suggests which items one might like to purchase based on previous purchases, and has reached nototiety through Netfilx challenge. Here, using matrix factorization is like to find individual pattern other than shared preference, and use statisital information to infer preferences. 

reference: Model Preferences Based on Paired Comparisons, Rankings, or Ratings. [here](file:///C:/Users/xiz933/Downloads/v48i10.pdf)
 

```{r,warning=FALSE, message=FALSE }
train_small <- train %>% 
    filter(movieId %in% unique(test$movieId) & userId %in% unique(test$userId)) %>%
    group_by(movieId) %>% 
    filter(n()>=5000) %>% 
    ungroup %>%
    group_by(userId) %>% 
    filter(n()>=250) %>% 
    ungroup  
```

## Problem 3A

To estimate this part of the model: $\sum_{j=1}^p b_{j,i} X_{i,k}$, 
we will consider the residuals $Y_{u,i}-\hat{\mu}-\hat{b}_i-\hat{b}_u$.
Create a column `resids` with these residuals for the 
`train_small` data set using the regularlized estimates you computed in 
Problem 2E.

```{r,warning=FALSE, message=FALSE }
train_small<-train_small %>%
             left_join(movie_bias_reg, by="movieId") %>%
             left_join(user_bias_reg, by="userId") %>%
             mutate(resids=rating-mu-b_i-b_u)

```



## Problem 3B

Now, we can construct a new matrix using the 
`spread()` function in `tidyr` (call it `Y`)
with movies in the rows and users in the columns. 
Use one of the join functions to merge the movie titles
from the 
[movies.csv file on GitHub](https://raw.githubusercontent.com/datasciencelabs/data/master/movies.csv). Remove the `genres` column (i.e. only 
keep the `title` column. Remove the year from the movie titles. 

Finally, create two objects (one with the `movieId` in `Y`,
call it `movie_ids`, and one with the `title` in `Y`,
call it `movie_titles`). These will be used in a little bit. Afterwards, remove the `movieID` and `title` columns in `Y`
using the `select()` function and then pipe the data frame 
to `as.matrix()` to convert to a matrix. 
Change all the missing data to a 0. There are better choices, 
but for simplicity we just use 0. 

```{r,warning=FALSE, message=FALSE }
library(tidyr)

# First, movie by row and user by column, to get movie_titles more easily, then transpose Y to make the final matrix user by row and movie by column
Y <- train_small%>%
     select(userId, movieId, resids)%>%
     spread(key=userId, value=resids) 

# join with movie title
filename <-'https://raw.githubusercontent.com/datasciencelabs/data/master/movies.csv'
movies <-read_csv(filename)
movies$title <- gsub("\\s\\([^)]*\\)","",movies$title) ##take out (year)
Y<- Y%>%left_join(movies, by="movieId") %>% select(-genres)

# make a string vector with titles in the same order as movies in Y
movie_titles <- Y$title
# make a numeric vector with movieId in the same order as movies in Y
movie_ids <-Y$movieId

# trandform Y into matrix
Y <- Y%>%
  select(-c(movieId, title)) %>%
  as.matrix() 
  
# replace NA with 0
Y[is.na(Y)] <-0
 
# transpose Y, matrix=user by movie
Y<-t(Y)

# assign movieId to Y column names
colnames(Y) <-movie_ids

```


Use the `prcomp()` function to obtain the PCs of `Y`. 
The PCs will be our estimates of the $X$. 
Use data visualization to explore if in fact the first PCs 
are grouping movies into meaningful groups. 

**Hint**: Use the `movie_titles` to label points. 

```{r,warning=FALSE, message=FALSE }
# Find the PCAs
pca <- prcomp(Y, center=FALSE, scale=FALSE)  

library(ggrepel)
# rotation is loading matrix, each row represents observation projection on that PC
tmp <- data.frame(pca$rotation, movie_title=movie_titles)  
 
# Use data visualization to explore if in fact the first 2 PCs are grouping movies into meaningful groups
tmp %>%  
  ggplot(aes(PC1,PC2)) + 
  geom_point() + 
  geom_text_repel(aes(PC1, PC2,label=movie_title),data = filter(tmp, PC1 < -0.1 | PC1 >0.1 | PC2 < -0.1 | PC2>0.1))+
  ggtitle("Visualize First Two PCAs")

```

**Your answer here**: 

> The first two PCAs seem not group movies into meaningful classes. We can see that Titanic, Forrest Gump, Jurassic Park, and Brave Heart are quite close to each other, however, from the content we would expect they are most likely attract different groups of fans. Therefore, only inlcude the first two PCs are not sufficient to separate groups.
 
 

## Problem 3C

If `pca` holds your `prcomp()` object with the PCs from `Y`, 
you can reconstruct and estimate with, say, $k$ = 20 
PCs using the following:

```{r,warning=FALSE, message=FALSE }
k <- 20
pred <- pca$x[,1:k] %*% t(pca$rotation[,1:k])
colnames(pred) <- colnames(Y)
 
```

Use the `gather()` and the `pred` object above to create a 
prediction using these data for the `test` data set. 
What RMSE do you get?

```{r,warning=FALSE, message=FALSE }
k <- 20
# x is the the rotated data, need to rotate back to original space
pred <- pca$x[,1:k] %*% t(pca$rotation[,1:k])
colnames(pred) <- colnames(Y)

# using PCA results to predict user and movie interaction
interaction <- 
    data.frame(userId = as.numeric(rownames(Y)), pred, check.names=FALSE) %>% 
    tbl_df %>%
    gather(key=movieId, value=b_ui, -userId) %>% # create a col for movieId, keep userId intact, each obs is per user per movie
    mutate(movieId = as.numeric(movieId))

#prediction
test_prediction <-test %>% 
  left_join(movie_bias_reg, by='movieId') %>% 
  left_join(user_bias_reg, by='userId') %>% 
  left_join(interaction, by=c('movieId','userId')) %>%
  replace_na(list(b_i=0, b_u=0, b_ui=0)) %>%
  mutate(predicted_ratings=mu+b_i+b_u+b_ui)

# RMSE
model4_RMSE <-RMSE(true_ratings =test_prediction$rating , predicted_ratings=test_prediction$predicted_ratings )
model4_RMSE

```

** Your answer here:**

> Include matrix decomposition as a way of modeling interaction between user and movies, I got RMSE 0.8579948, improved a lot from naive guess, and also significantly from linear relationship between user and movie.



## Problem 3D

We tried out 5 different models. Report the RMSE for all 5.
If you are so inclined, including others such as what you 
get with different $\lambda$, $\alpha$ or $k$.

```{r,warning=FALSE, message=FALSE }
RMSE_results <-bind_rows(RMSE_results, data_frame(Method="Matrix Decomposition", RMSE=model4_RMSE))

# View Model RMSE table
library(knitr)
RMSE_results %>% kable
```


-----






## Problem 4

Download [a test data set available on GitHub here](https://github.com/datasciencelabs/data/blob/master/movielens-test.csv.gz). 
Make predictions to fill in the `NA`s and save a
file with the same format but with the ratings filled in. 
Submit this as a `.csv` file as part of your homework with your name in
in the file name. 

```{r,warning=FALSE, message=FALSE }
test <-read_csv('movielens-test.csv')

#prediction
test <-test %>% 
  left_join(movie_bias_reg, by='movieId') %>% 
  left_join(user_bias_reg, by='userId') %>% 
  left_join(interaction, by=c('movieId','userId')) %>%
  replace_na(list(b_i=0, b_u=0, b_ui=0)) %>%
  mutate(rating=mu+b_i+b_u+b_ui)

write.csv(x=test, file="XinerZhou.csv")
```



